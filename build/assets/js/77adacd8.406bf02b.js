"use strict";(globalThis.webpackChunkdocusaurus_init_temp=globalThis.webpackChunkdocusaurus_init_temp||[]).push([[194],{1253:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/Physical-AI-and-robotic/docs/intro","label":"Introduction to Physical AI & Humanoid Robotics","docId":"intro","unlisted":false},{"type":"link","href":"/Physical-AI-and-robotic/docs/importance-of-physical-ai","label":"The Growing Importance of Physical AI","docId":"importance-of-physical-ai","unlisted":false},{"type":"category","label":"Module 01: ROS2 Fundamentals","items":[{"type":"link","href":"/Physical-AI-and-robotic/docs/module-ros2/chapter-1","label":"ROS 2 Fundamentals: Week 1 - Getting Started","docId":"module-ros2/chapter-1","unlisted":false},{"type":"link","href":"/Physical-AI-and-robotic/docs/module-ros2/chapter-2","label":"ROS 2 Fundamentals: Week 2 - Nodes, Topics, and Messages","docId":"module-ros2/chapter-2","unlisted":false},{"type":"link","href":"/Physical-AI-and-robotic/docs/module-ros2/chapter-3","label":"ROS 2 Fundamentals: Week 3 - Services and Actions","docId":"module-ros2/chapter-3","unlisted":false},{"type":"link","href":"/Physical-AI-and-robotic/docs/module-ros2/chapter-4","label":"ROS 2 Fundamentals: Week 4 - Parameters, Launch Files, and Best Practices","docId":"module-ros2/chapter-4","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/Physical-AI-and-robotic/docs/category/ros2-fundamentals"},{"type":"category","label":"Module 02: Digital Twin Prototyping","items":[{"type":"link","href":"/Physical-AI-and-robotic/docs/module-digital-twin/gazebo-integration","label":"Digital Twin and Simulation: Gazebo Integration","docId":"module-digital-twin/gazebo-integration","unlisted":false},{"type":"link","href":"/Physical-AI-and-robotic/docs/module-digital-twin/chapter-2","label":"Digital Twin and Simulation: Advanced Gazebo Features","docId":"module-digital-twin/chapter-2","unlisted":false},{"type":"link","href":"/Physical-AI-and-robotic/docs/module-digital-twin/chapter-3","label":"Digital Twin and Simulation: Digital Twin Best Practices and Applications","docId":"module-digital-twin/chapter-3","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/Physical-AI-and-robotic/docs/category/digital-twin-prototyping"},{"type":"category","label":"Module 03: AI Robot Brains","items":[{"type":"link","href":"/Physical-AI-and-robotic/docs/module-ai-robot-brain/chapter-1","label":"AI Robot Brains: Chapter 1 - Perception and Sensing","docId":"module-ai-robot-brain/chapter-1","unlisted":false},{"type":"link","href":"/Physical-AI-and-robotic/docs/module-ai-robot-brain/chapter-2","label":"AI Robot Brains: Chapter 2 - Navigation and Control","docId":"module-ai-robot-brain/chapter-2","unlisted":false},{"type":"link","href":"/Physical-AI-and-robotic/docs/module-ai-robot-brain/chapter-3","label":"AI Robot Brains: Chapter 3 - Human-Robot Interaction and Learning","docId":"module-ai-robot-brain/chapter-3","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/Physical-AI-and-robotic/docs/category/ai-robot-brains"},{"type":"category","label":"Module 04: Visual Language & Action (VLA) Pipelines","items":[{"type":"link","href":"/Physical-AI-and-robotic/docs/module-vla-pipeline/chapter-1","label":"Visual Language & Action (VLA) Pipelines: Chapter 1 - Foundations of Visual-Language Models","docId":"module-vla-pipeline/chapter-1","unlisted":false},{"type":"link","href":"/Physical-AI-and-robotic/docs/module-vla-pipeline/chapter-2","label":"Visual Language & Action (VLA) Pipelines: Chapter 2 - Grounding Language in Action","docId":"module-vla-pipeline/chapter-2","unlisted":false},{"type":"link","href":"/Physical-AI-and-robotic/docs/module-vla-pipeline/chapter-3","label":"Visual Language & Action (VLA) Pipelines: Chapter 3 - Advanced VLA Applications and Research Frontiers","docId":"module-vla-pipeline/chapter-3","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/Physical-AI-and-robotic/docs/category/vla-pipelines"},{"type":"link","href":"/Physical-AI-and-robotic/docs/hardware-requirements","label":"Hardware Requirements for Physical AI & Robotics","docId":"hardware-requirements","unlisted":false}]},"docs":{"hardware-requirements":{"id":"hardware-requirements","title":"Hardware Requirements for Physical AI & Robotics","description":"Developing and deploying Physical AI and humanoid robotics solutions often requires specific hardware considerations. This chapter outlines typical hardware requirements, categorized for different stages of development and deployment.","sidebar":"tutorialSidebar"},"importance-of-physical-ai":{"id":"importance-of-physical-ai","title":"The Growing Importance of Physical AI","description":"Physical AI is rapidly emerging as a transformative field, bridging the gap between artificial intelligence and the tangible world. Its significance stems from its ability to extend AI beyond data centers and into real-world applications, directly impacting industries, daily life, and scientific discovery.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Introduction to Physical AI & Humanoid Robotics","description":"Welcome to the Physical AI & Humanoid Robotics book!","sidebar":"tutorialSidebar"},"module-ai-robot-brain/chapter-1":{"id":"module-ai-robot-brain/chapter-1","title":"AI Robot Brains: Chapter 1 - Perception and Sensing","description":"The ability of a robot to understand its environment is paramount to its intelligence. This chapter focuses on the fundamental aspects of perception and sensing, which form the \\"eyes and ears\\" of an AI robot brain.","sidebar":"tutorialSidebar"},"module-ai-robot-brain/chapter-2":{"id":"module-ai-robot-brain/chapter-2","title":"AI Robot Brains: Chapter 2 - Navigation and Control","description":"With a foundational understanding of robot perception, this chapter moves to how robots move and interact within their environment. We will explore key concepts in navigation, motion planning, and control, which dictate a robot\'s physical actions.","sidebar":"tutorialSidebar"},"module-ai-robot-brain/chapter-3":{"id":"module-ai-robot-brain/chapter-3","title":"AI Robot Brains: Chapter 3 - Human-Robot Interaction and Learning","description":"The ultimate goal for many physical AI systems, especially humanoid robots, is seamless interaction with humans and the ability to learn new skills. This chapter explores the advanced topics of human-robot interaction (HRI) and various learning paradigms that enable robots to become more intelligent and adaptable partners.","sidebar":"tutorialSidebar"},"module-digital-twin/chapter-2":{"id":"module-digital-twin/chapter-2","title":"Digital Twin and Simulation: Advanced Gazebo Features","description":"Building upon the basics of Gazebo integration, this chapter explores more advanced features that enable highly realistic and complex robot simulations. Mastering these features is key to developing robust digital twins.","sidebar":"tutorialSidebar"},"module-digital-twin/chapter-3":{"id":"module-digital-twin/chapter-3","title":"Digital Twin and Simulation: Digital Twin Best Practices and Applications","description":"Having explored Gazebo\'s capabilities, this chapter focuses on best practices for designing and utilizing digital twins effectively. We will also look at real-world applications and the future of digital twin technology in physical AI.","sidebar":"tutorialSidebar"},"module-digital-twin/gazebo-integration":{"id":"module-digital-twin/gazebo-integration","title":"Digital Twin and Simulation: Gazebo Integration","description":"Simulation plays a crucial role in robotics development, offering a safe, cost-effective, and efficient environment for testing algorithms, prototyping designs, and training AI models. Gazebo is one of the most widely used 3D robot simulators, providing realistic physics, sensor models, and a robust interface with ROS 2.","sidebar":"tutorialSidebar"},"module-ros2/chapter-1":{"id":"module-ros2/chapter-1","title":"ROS 2 Fundamentals: Week 1 - Getting Started","description":"Welcome to the first week of our ROS 2 module! This week, we will lay the groundwork for understanding and working with the Robot Operating System 2 (ROS 2). ROS 2 is a flexible framework for writing robot software. It\'s a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behaviors across a wide variety of robotic platforms.","sidebar":"tutorialSidebar"},"module-ros2/chapter-2":{"id":"module-ros2/chapter-2","title":"ROS 2 Fundamentals: Week 2 - Nodes, Topics, and Messages","description":"This week, we dive deeper into the core communication mechanisms of ROS 2: Nodes, Topics, and Messages. Understanding how these components interact is fundamental to building any ROS 2 application.","sidebar":"tutorialSidebar"},"module-ros2/chapter-3":{"id":"module-ros2/chapter-3","title":"ROS 2 Fundamentals: Week 3 - Services and Actions","description":"Building upon topics and messages, this week we explore two other crucial communication paradigms in ROS 2: Services and Actions. These mechanisms allow for more structured and complex interactions between nodes.","sidebar":"tutorialSidebar"},"module-ros2/chapter-4":{"id":"module-ros2/chapter-4","title":"ROS 2 Fundamentals: Week 4 - Parameters, Launch Files, and Best Practices","description":"In our final week of ROS 2 fundamentals, we cover how to configure your nodes dynamically using parameters, orchestrate multiple nodes with launch files, and discuss essential best practices for efficient and maintainable ROS 2 development.","sidebar":"tutorialSidebar"},"module-vla-pipeline/chapter-1":{"id":"module-vla-pipeline/chapter-1","title":"Visual Language & Action (VLA) Pipelines: Chapter 1 - Foundations of Visual-Language Models","description":"The integration of visual perception with natural language understanding is a pivotal step towards truly intelligent robots. This chapter introduces the foundational concepts of Visual-Language Models (VLMs) and how they enable robots to interpret and act upon human commands expressed in natural language, grounded in visual observations.","sidebar":"tutorialSidebar"},"module-vla-pipeline/chapter-2":{"id":"module-vla-pipeline/chapter-2","title":"Visual Language & Action (VLA) Pipelines: Chapter 2 - Grounding Language in Action","description":"Building on the foundations of Visual-Language Models, this chapter delves into how robots can translate natural language commands, grounded in visual perception, into concrete physical actions. This process involves action planning, skill execution, and robust feedback mechanisms.","sidebar":"tutorialSidebar"},"module-vla-pipeline/chapter-3":{"id":"module-vla-pipeline/chapter-3","title":"Visual Language & Action (VLA) Pipelines: Chapter 3 - Advanced VLA Applications and Research Frontiers","description":"This final chapter delves into cutting-edge applications of Visual Language and Action (VLA) pipelines and explores the exciting research frontiers pushing the boundaries of what robots can achieve through natural language interaction.","sidebar":"tutorialSidebar"}}}}')}}]);