"use strict";(globalThis.webpackChunkdocusaurus_init_temp=globalThis.webpackChunkdocusaurus_init_temp||[]).push([[340],{6662:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/Physical-AI-and-Robotics/docs/intro","label":"Introduction to Physical AI & Humanoid Robotics","docId":"intro","unlisted":false},{"type":"link","href":"/Physical-AI-and-Robotics/docs/importance-of-physical-ai","label":"The Growing Importance of Physical AI","docId":"importance-of-physical-ai","unlisted":false},{"type":"category","label":"Module 01: ROS2 Fundamentals","items":[{"type":"link","href":"/Physical-AI-and-Robotics/docs/module-ros2/chapter-1","label":"Get Started - week 01","docId":"module-ros2/chapter-1","unlisted":false},{"type":"link","href":"/Physical-AI-and-Robotics/docs/module-ros2/chapter-2","label":"Nodes, Topics, and Messages - Week 02","docId":"module-ros2/chapter-2","unlisted":false},{"type":"link","href":"/Physical-AI-and-Robotics/docs/module-ros2/chapter-3","label":"Services and Actions - week 03","docId":"module-ros2/chapter-3","unlisted":false},{"type":"link","href":"/Physical-AI-and-Robotics/docs/module-ros2/chapter-4","label":"Parameters, Launch Files, and Best Practices - week 04","docId":"module-ros2/chapter-4","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/Physical-AI-and-Robotics/docs/category/ros2-fundamentals"},{"type":"category","label":"Module 02: Digital Twin Prototyping","items":[{"type":"link","href":"/Physical-AI-and-Robotics/docs/module-digital-twin/gazebo-integration","label":"Gazebo Integration - week 05","docId":"module-digital-twin/gazebo-integration","unlisted":false},{"type":"link","href":"/Physical-AI-and-Robotics/docs/module-digital-twin/chapter-2","label":"Advanced Gazebo Features - week 06","docId":"module-digital-twin/chapter-2","unlisted":false},{"type":"link","href":"/Physical-AI-and-Robotics/docs/module-digital-twin/chapter-3","label":"Digital Twin Best Practices and Applications - week 07","docId":"module-digital-twin/chapter-3","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/Physical-AI-and-Robotics/docs/category/digital-twin-prototyping"},{"type":"category","label":"Module 03: AI Robot Brains","items":[{"type":"link","href":"/Physical-AI-and-Robotics/docs/module-ai-robot-brain/chapter-1","label":"Perception and Sensing - week 08","docId":"module-ai-robot-brain/chapter-1","unlisted":false},{"type":"link","href":"/Physical-AI-and-Robotics/docs/module-ai-robot-brain/chapter-2","label":"Navigation and Control - week 09","docId":"module-ai-robot-brain/chapter-2","unlisted":false},{"type":"link","href":"/Physical-AI-and-Robotics/docs/module-ai-robot-brain/chapter-3","label":"Human-Robot Interaction and Learning - week 10","docId":"module-ai-robot-brain/chapter-3","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/Physical-AI-and-Robotics/docs/category/ai-robot-brains"},{"type":"category","label":"Module 04: Visual Language & Action (VLA) Pipelines","items":[{"type":"link","href":"/Physical-AI-and-Robotics/docs/module-vla-pipeline/chapter-1","label":"Foundations of Visual-Language Models week 11","docId":"module-vla-pipeline/chapter-1","unlisted":false},{"type":"link","href":"/Physical-AI-and-Robotics/docs/module-vla-pipeline/chapter-2","label":"Grounding Language in Action - week 12","docId":"module-vla-pipeline/chapter-2","unlisted":false},{"type":"link","href":"/Physical-AI-and-Robotics/docs/module-vla-pipeline/chapter-3","label":"Advanced VLA Applications and Research Frontiers - week 13","docId":"module-vla-pipeline/chapter-3","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/Physical-AI-and-Robotics/docs/category/vla-pipelines"},{"type":"link","href":"/Physical-AI-and-Robotics/docs/hardware-requirements","label":"Hardware Requirements for Physical AI & Robotics","docId":"hardware-requirements","unlisted":false}]},"docs":{"hardware-requirements":{"id":"hardware-requirements","title":"Hardware Requirements for Physical AI & Robotics","description":"Developing and deploying Physical AI and humanoid robotics solutions often requires specific hardware considerations. This chapter outlines typical hardware requirements, categorized for different stages of development and deployment.","sidebar":"tutorialSidebar"},"importance-of-physical-ai":{"id":"importance-of-physical-ai","title":"The Growing Importance of Physical AI","description":"Physical AI is rapidly emerging as a transformative field, bridging the gap between artificial intelligence and the tangible world. Its significance stems from its ability to extend AI beyond data centers and into real-world applications, directly impacting industries, daily life, and scientific discovery.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Introduction to Physical AI & Humanoid Robotics","description":"Welcome to the Physical AI & Humanoid Robotics book!","sidebar":"tutorialSidebar"},"module-ai-robot-brain/chapter-1":{"id":"module-ai-robot-brain/chapter-1","title":"Perception and Sensing - week 08","description":"The ability of a robot to understand its environment is paramount to its intelligence. This chapter focuses on the fundamental aspects of perception and sensing, which form the \\"eyes and ears\\" of an AI robot brain.","sidebar":"tutorialSidebar"},"module-ai-robot-brain/chapter-2":{"id":"module-ai-robot-brain/chapter-2","title":"Navigation and Control - week 09","description":"With a foundational understanding of robot perception, this chapter moves to how robots move and interact within their environment. We will explore key concepts in navigation, motion planning, and control, which dictate a robot\'s physical actions.","sidebar":"tutorialSidebar"},"module-ai-robot-brain/chapter-3":{"id":"module-ai-robot-brain/chapter-3","title":"Human-Robot Interaction and Learning - week 10","description":"The ultimate goal for many physical AI systems, especially humanoid robots, is seamless interaction with humans and the ability to learn new skills. This chapter explores the advanced topics of human-robot interaction (HRI) and various learning paradigms that enable robots to become more intelligent and adaptable partners.","sidebar":"tutorialSidebar"},"module-digital-twin/chapter-2":{"id":"module-digital-twin/chapter-2","title":"Advanced Gazebo Features - week 06","description":"Building upon the basics of Gazebo integration, this chapter explores more advanced features that enable highly realistic and complex robot simulations. Mastering these features is key to developing robust digital twins.","sidebar":"tutorialSidebar"},"module-digital-twin/chapter-3":{"id":"module-digital-twin/chapter-3","title":"Digital Twin Best Practices and Applications - week 07","description":"Having explored Gazebo\'s capabilities, this chapter focuses on best practices for designing and utilizing digital twins effectively. We will also look at real-world applications and the future of digital twin technology in physical AI.","sidebar":"tutorialSidebar"},"module-digital-twin/gazebo-integration":{"id":"module-digital-twin/gazebo-integration","title":"Gazebo Integration - week 05","description":"Simulation plays a crucial role in robotics development, offering a safe, cost-effective, and efficient environment for testing algorithms, prototyping designs, and training AI models. Gazebo is one of the most widely used 3D robot simulators, providing realistic physics, sensor models, and a robust interface with ROS 2.","sidebar":"tutorialSidebar"},"module-ros2/chapter-1":{"id":"module-ros2/chapter-1","title":"Get Started - week 01","description":"Welcome to the first week of our ROS 2 module! This week, we will lay the groundwork for understanding and working with the Robot Operating System 2 (ROS 2). ROS 2 is a flexible framework for writing robot software. It\'s a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behaviors across a wide variety of robotic platforms.","sidebar":"tutorialSidebar"},"module-ros2/chapter-2":{"id":"module-ros2/chapter-2","title":"Nodes, Topics, and Messages - Week 02","description":"This week, we dive deeper into the core communication mechanisms of ROS 2:","sidebar":"tutorialSidebar"},"module-ros2/chapter-3":{"id":"module-ros2/chapter-3","title":"Services and Actions - week 03","description":"Building upon topics and messages, this week we explore two other crucial communication paradigms in ROS 2: Services and Actions. These mechanisms allow for more structured and complex interactions between nodes.","sidebar":"tutorialSidebar"},"module-ros2/chapter-4":{"id":"module-ros2/chapter-4","title":"Parameters, Launch Files, and Best Practices - week 04","description":"In our final week of ROS 2 fundamentals, we cover how to configure your nodes dynamically using parameters, orchestrate multiple nodes with launch files, and discuss essential best practices for efficient and maintainable ROS 2 development.","sidebar":"tutorialSidebar"},"module-vla-pipeline/chapter-1":{"id":"module-vla-pipeline/chapter-1","title":"Foundations of Visual-Language Models week 11","description":"The integration of visual perception with natural language understanding is a pivotal step towards truly intelligent robots. This chapter introduces the foundational concepts of Visual-Language Models (VLMs) and how they enable robots to interpret and act upon human commands expressed in natural language, grounded in visual observations.","sidebar":"tutorialSidebar"},"module-vla-pipeline/chapter-2":{"id":"module-vla-pipeline/chapter-2","title":"Grounding Language in Action - week 12","description":"Building on the foundations of Visual-Language Models, this chapter delves into how robots can translate natural language commands, grounded in visual perception, into concrete physical actions. This process involves action planning, skill execution, and robust feedback mechanisms.","sidebar":"tutorialSidebar"},"module-vla-pipeline/chapter-3":{"id":"module-vla-pipeline/chapter-3","title":"Advanced VLA Applications and Research Frontiers - week 13","description":"This final chapter delves into cutting-edge applications of Visual Language and Action (VLA) pipelines and explores the exciting research frontiers pushing the boundaries of what robots can achieve through natural language interaction.","sidebar":"tutorialSidebar"}}}}')}}]);